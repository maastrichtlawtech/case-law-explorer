import logging
from os import makedirs
from os.path import dirname, abspath, join, exists

"""
Purpose of script:
a)  Define directory paths.
b)  Define the terminology to be used throughout all data processing steps.
    The original terms used in the raw data of "Rechtspraak" and "Legal intelligence" are mapped to each other
    and replaced by a global label.
    Variable names correspond to the original terms and are denoted with the prefix of their source (RS = Rechtspraak, 
    LI = Legal Intelligence). Variables without prefix don't exist in the raw data and are generated by script.
"""

# local data folder structure
DIR_ROOT = dirname(dirname(dirname(abspath(__file__))))
DIR_DATA = join(DIR_ROOT, 'data')
DIR_LOGS = join(DIR_ROOT, 'logs')
DIR_DATA_RAW = join(DIR_DATA, 'raw')
DIR_DATA_RECHTSPRAAK = join(DIR_DATA, 'Rechtspraak')
DIR_DATA_PROCESSED = join(DIR_DATA, 'processed')
DIR_DATA_FULL_TEXT = join(DIR_DATA, 'full_text')

# data file names
DIR_RECHTSPRAAK = join(DIR_DATA, 'Rechtspraak', 'OpenDataUitspraken')
DIR_ECHR = join(DIR_DATA, 'echr')
CELLAR_DIR = join(DIR_DATA, 'cellar')
CSV_RS_CASES = 'RS_cases.csv'  # metadata of RS cases
CSV_CELLAR_CASES = 'cellar_csv_data.csv'  # Metadata of CELLAR cases
TXT_CELLAR_EDGES = 'cellar_edges.txt'
TXT_CELLAR_NODES = 'cellar_nodes.txt'
TXT_ECHR_EDGES = 'ECHR_edges.txt'
TXT_ECHR_NODES = 'ECHR_nodes.txt'
CSV_RS_INDEX = 'RS_index.csv'  # eclis, decision dates and relations of RS cases and opinions
CSV_DDB_ECLIS_FAILED = 'DDB_eclis_failed.csv'
CSV_ECHR_CASES = 'ECHR_metadata.csv'
CSV_ECHR_CASES_NODES = 'ECHR_nodes.csv'
CSV_ECHR_CASES_EDGES = "ECHR_edges.csv"
JSON_FULL_TEXT_CELLAR = join(DIR_DATA_FULL_TEXT, 'cellar_full_text.json')
JSON_FULL_TEXT_ECHR = join(DIR_DATA_FULL_TEXT, 'ECHR_full_text.json')


# raw data:
def get_path_raw(file_name):
    return join(DIR_DATA_RAW, file_name)


# processed data
def get_path_processed(file_name):
    return join(DIR_DATA_PROCESSED, file_name.split('.csv')[0] + '_clean.csv')


class Storage:
    def __init__(self):
        self.location = 'local'
        self.pipeline_input_path = None
        self.pipeline_output_paths = None
        logging.info(f'Setting up {self.location} storage ...')
        self._setup()

    def _setup(self):
        # create local data folder structure, if it doesn't exist yet
        for d in [DIR_DATA_RAW, DIR_DATA_PROCESSED, DIR_DATA_FULL_TEXT]:
            makedirs(d, exist_ok=True)
        logging.info('Storage set up.')

    def setup_pipeline(self, output_paths=None, input_path=None):
        self.pipeline_input_path = input_path
        self.pipeline_output_paths = output_paths

        # fetch output data
        if self.pipeline_output_paths:
            logging.info(f'Fetching output data from {self.location} storage ...')
            for path in self.pipeline_output_paths:
                if exists(path):
                    logging.warning(f'{path} exists locally! Move/rename local file before starting pipeline.')
                    raise Exception(path + ' exists locally! Move/rename local file before starting pipeline.')

        # fetch input data
        if self.pipeline_input_path:
            if not exists(input_path):
                logging.warning(f'{input_path} does not exist locally!')
                raise Exception(input_path + ' does not exist locally!')
