from os.path import basename, dirname, abspath, join, exists, relpath, isfile, getsize, getmtime
from os import makedirs, getenv, listdir, walk
from concurrent.futures import ThreadPoolExecutor
import hashlib
import re
import boto3
from botocore.exceptions import ClientError
import logging
from dotenv import load_dotenv
load_dotenv()
import sys
from datetime import date, datetime

"""
Purpose of script:
a)  Define directory paths.
b)  Define the terminology to be used throughout all data processing steps.
    The original terms used in the raw data of "Rechtspraak" and "Legal intelligence" are mapped to each other
    and replaced by a global label.
    Variable names correspond to the original terms and are denoted with the prefix of their source (RS = Rechtspraak, 
    LI = Legal Intelligence). Variables without prefix don't exist in the raw data and are generated by script.
"""

# local data folder structure
DIR_ROOT = dirname(dirname(abspath(__file__)))
DIR_DATA = join(DIR_ROOT, 'data')
DIR_DATA_RAW = join(DIR_DATA, 'raw')
DIR_DATA_PROCESSED = join(DIR_DATA, 'processed')

# data file names
DIR_RECHTSPRAAK = join(DIR_DATA, 'Rechtspraak', 'OpenDataUitspraken')
DIR_ECHR = join(DIR_DATA, 'echr')
CELLAR_DIR = join(DIR_DATA, 'cellar')
CSV_OPENDATA_INDEX = DIR_RECHTSPRAAK + '_index.csv'         # eclis and decision dates of OpenDataUitspraken files
CSV_RS_CASES = 'RS_cases.csv'                               # metadata of RS cases
CSV_RS_OPINIONS = 'RS_opinions.csv'                         # metadata of RS opinions
CSV_RS_INDEX = 'RS_index.csv'                               # eclis, decision dates and relations of RS cases and opinions
CSV_LI_CASES = 'LI_cases.csv'                               # metadata of LI cases
CSV_CASE_CITATIONS = 'caselaw_citations.csv'                # citations of RS cases and opinions
CSV_LEGISLATION_CITATIONS = 'legislation_citations.csv'     # cited legislation of RS cases and opinions
CSV_LIDO_ECLIS_FAILED = 'LIDO_eclis_failed.csv'
CSV_DDB_ECLIS_FAILED = 'DDB_eclis_failed.csv'
CSV_OS_ECLIS_FAILED = 'OS_eclis_failed.csv'
CSV_ECHR_CASES = join(DIR_ECHR, 'ECHR_metadata.csv')


# raw data:
def get_path_raw(file_name):
    return join(DIR_DATA_RAW, file_name)


# processed data
def get_path_processed(file_name):
    return join(DIR_DATA_PROCESSED, file_name.split('.csv')[0] + '_clean.csv')

def md5sum_file(file_path):
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as local_file:
        for chunk in iter(lambda: local_file.read(4096), b""):
            hash_md5.update(chunk)

    return '"' + hash_md5.hexdigest() + '"'

class FileStats:
    def __init__(self, time, size, digest):
        self.date = time
        self.size = size
        self.digest = digest

    def __eq__(self, other):
        return (self.date == other.date and self.size == other.size
                and self.digest == other.digest)

def stats_from_path(filepath):
    return FileStats(getmtime(filepath), getsize(filepath), md5sum_file(filepath))

def stats_from_aws_obj_summary(obj):
    return FileStats(obj.last_modified, obj.size, obj.e_tag)

class Storage:
    def __init__(self, location):
        if location not in ('local', 'aws'):
            print('Storage location must be either "local" or "aws". Setting to "local".')
            location = 'local'
        self.location = location
        self.s3_bucket_name = getenv('S3_BUCKET_NAME')
        self.s3_bucket = None
        self.s3_client = None
        self.pipeline_input_path = None
        self.pipeline_output_paths = None
        self.pipeline_last_updated = date(1900, 1, 1)

        print(f'\nSetting up {self.location} storage ...')
        self._setup()

    def _setup(self):
        # create local data folder structure, if it doesn't exist yet
        for d in [dirname(DIR_RECHTSPRAAK), DIR_DATA_RAW, DIR_DATA_PROCESSED, CELLAR_DIR]:
            makedirs(d, exist_ok=True)

        if self.location == 'aws':
            # create an S3 bucket in the region of the configured AWS IAM user account
            try:
                region = getenv('AWS_REGION')
                s3_client = boto3.client('s3', region_name=region)
                aws_location = {'LocationConstraint': region}
                s3_client.create_bucket(Bucket=self.s3_bucket_name, CreateBucketConfiguration=aws_location)
            except ClientError as e:
                if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou' or \
                        e.response['Error']['Code'] == 'BucketAlreadyExists':
                    logging.warning(f'S3 bucket "{self.s3_bucket_name}" already exists. Content might be overwritten.')
                else:
                    raise e
            self.s3_bucket = boto3.resource('s3').Bucket(self.s3_bucket_name)
            self.s3_client = boto3.client('s3')
        print('Storage set up.')

    def setup_pipeline(self, output_paths=None, input_path=None):
        self.pipeline_input_path = input_path
        self.pipeline_output_paths = output_paths

        # fetch output data
        if self.pipeline_output_paths:
            print(f'\nFetching output data from {self.location} storage ...')
            for path in self.pipeline_output_paths:
                if exists(path):
                    logging.error(f'{path} exists locally! Move/rename local file before starting pipeline.')
                    sys.exit(2)
                if path.endswith('.csv'):
                    self.fetch_data([path])

            # retrieve output date of last update
            self.pipeline_last_updated = self.fetch_last_updated(self.pipeline_output_paths)

        # fetch input data
        if self.pipeline_input_path:
            print(f'\nFetching input data from {self.location} storage ...')
            self.fetch_data([self.pipeline_input_path])
            # retrieve input date of last update
            last_updated_input = self.fetch_last_updated([self.pipeline_input_path])

            # if output date of last update after input date of last update: need to update input first
            if last_updated_input < self.pipeline_last_updated:
                logging.error(f'Input data {basename(self.pipeline_input_path)} is older than output data. '
                              f'Please update input data first.')
                sys.exit(2)

    def finish_pipeline(self):
        if self.pipeline_output_paths:
            self.upload_data(self.pipeline_output_paths)

    def fetch_data(self, paths):
        def not_found(file_path):
            msg = file_path + ' does not exist! Consider switching storage location' \
                         ' or re-running earlier steps of the pipeline.'
            if file_path == self.pipeline_input_path:
                logging.error(msg)
                sys.exit(2)
            else:
                logging.warning(msg)

        def fetch_data_local(file_path):
            # exit if input does not exist
            if not exists(file_path):
                not_found(file_path)
            else:
                print('Local data ready.')

        def fetch_data_aws(objs, file_path):
            if exists(file_path):
                if objs[file_path] != stats_from_path(file_path):
                    logging.error(f'{file_path} exists locally with different data! Move/rename local file before fetching data from aws.')
                    sys.exit(2)
                print(f'{basename(file_path)} already downloaded.')
                return

            try:
                makedirs(dirname(file_path), exist_ok=True)
                self.s3_bucket.download_file(relpath(file_path, DIR_ROOT), file_path)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    not_found(file_path)

            print(f'{basename(file_path)} fetched.')

        if self.location == 'local':
            for path in paths:
                fetch_data_local(path)
        elif self.location == 'aws':
            with ThreadPoolExecutor() as executor:
                for path in paths:
                    rel_dir = relpath(path, DIR_ROOT)
                    objs = {}
                    for obj in self.s3_bucket.objects.filter(Prefix=rel_dir):
                        objs[obj.key] = stats_from_aws_obj_summary(obj)

                    executor.map(lambda name: fetch_data_aws(objs, name), objs)

    def upload_data(self, paths):
        def upload_to_aws(objs, rel_dir, file_name):
            file_path = join(rel_dir, file_name)
            if file_path in objs:
                file_size = getsize(file_path)
                if file_size == objs[file_path].size:
                    digest = md5sum_file(join(DIR_ROOT, file_path))
                    if digest == objs[file_path].digest:
                        return

                    print("Different file contents for:", file_path)
                else:
                    print("Different file sizes for:", file_path)

            try:
                self.s3_bucket.upload_file(join(DIR_ROOT, file_path), file_path)
            except ClientError as e:
                logging.error(e)

        def handle_dir(executor, root, files):
            rel_dir = relpath(root, DIR_ROOT)
            objs = {}
            for obj in self.s3_bucket.objects.filter(Prefix=rel_dir):
                objs[obj.key] = stats_from_aws_obj_summary(obj)

            executor.map(lambda name: upload_to_aws(objs, rel_dir, name), files)

        if self.location == 'aws':
            with ThreadPoolExecutor() as executor:
                for path in paths:
                    for root, _, files in walk(path):
                        if len(files) == 0:
                            continue

                        executor.submit(handle_dir, executor, root, files)

                    print(basename(path), 'loaded to aws.')
        else:
            print('Local data updated.')

    def fetch_last_updated(self, paths):
        def date_map(file_path):
            default = ('date_decision', lambda x: date.fromisoformat(x))
            d_map = {
                get_path_raw(CSV_LI_CASES): ('EnactmentDate', lambda x: datetime.strptime(x, "%Y%m%d").date())
            }
            return d_map.get(file_path, default)

        def default_date(file_path):
            print(f'Setting start date of {basename(file_path)} to 1900-01-01.')
            return date(1900, 1, 1)

        def last_updated(file_path):
            if file_path == DIR_RECHTSPRAAK:
                self.fetch_data([CSV_OPENDATA_INDEX])
                file_path = CSV_OPENDATA_INDEX

            if re.match(rf'^{CELLAR_DIR}/.*\.json$', file_path):
                if self.location == 'local':
                    # Go through existing JSON files and use their filename to determine when the last 
                    # update was.
                    # Alternatively, this could be switched to loading all the JSONs and checking the
                    # max last modification date.
                    new_date = datetime(1900, 1, 1)
                    for filename in listdir(CELLAR_DIR):
                        match = re.match(
                            r'^(\d{4})-(\d{2})-(\d{2})T(\d{2})_(\d{2})\_(\d{2})\.json$', filename)
                        if not match:
                            continue

                        new_date = max(
                            new_date,
                            datetime(
                                int(match[1]), int(match[2]), int(match[3]),
                                int(match[4]), int(match[5]), int(match[6])
                            )
                        )
                    return new_date

            if file_path.endswith('.csv'):
                import pandas as pd
                try:
                    date_name, date_function = date_map(file_path)
                    df = pd.read_csv(file_path, usecols=[date_name], dtype=str)
                    return max(df[date_name].apply(date_function))
                except FileNotFoundError:
                    logging.warning(file_path + ' not found.')
                    return default_date(file_path)

            logging.warning(basename(file_path) + ' is not a .csv file.')
            return default_date(file_path)

        last_updated_dates = []
        for path in paths:
            this_last_updated = last_updated(path)
            print(f'- Last updated {basename(path)}:\t {this_last_updated}')
            last_updated_dates.append(this_last_updated)

        return min(last_updated_dates)
